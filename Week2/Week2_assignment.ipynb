{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'n') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    import nltk\n",
    "    unique_tokens = len(set(text1))\n",
    "    total_tokens = len(text1)\n",
    "    ratio = unique_tokens / total_tokens\n",
    "    return ratio\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    dist_word = nltk.FreqDist(moby_tokens)\n",
    "    whale_tokens = dist_word['whale']\n",
    "    whale_tokens2 = dist_word['Whale']\n",
    "    total_tokens = len(text1)\n",
    "    ratio = (whale_tokens+whale_tokens2)/total_tokens\n",
    "    return ratio\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    dist_word = nltk.FreqDist(moby_tokens)\n",
    "    list_new = []\n",
    "    for w in dist_word.keys():\n",
    "        list_new.append(dist_word[w])\n",
    "    list_new.sort(reverse = True)\n",
    "    a = sorted(dist_word.items(), key = lambda kv: kv[1], reverse = True)\n",
    "    return a[:20]\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    \n",
    "    dist_new = nltk.FreqDist(moby_tokens)\n",
    "    new_list = [w for w in dist_new.keys() if len(w) > 5 and dist_new[w] > 150]\n",
    "    new_list.sort()\n",
    "    return new_list\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    dist_new = nltk.FreqDist(moby_tokens)\n",
    "    sorted_long_word = sorted(dist_new.items(), key=lambda kv: len(kv[0]), reverse = True)\n",
    "    (Longest_word, Length) = (sorted_long_word[0][0], len(sorted_long_word[0][0]))\n",
    "    return (Longest_word, Length)\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    dist_new = nltk.FreqDist(moby_tokens)\n",
    "    list_new = []\n",
    "    for w in dist_new.keys():\n",
    "        if w.isalpha():\n",
    "            if dist_new[w] > 2000:\n",
    "                list_new.append((dist_new[w], w))\n",
    "    list_new.sort(reverse = True)\n",
    "    return list_new\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    sents_moby = nltk.sent_tokenize(moby_raw)\n",
    "    a = 0\n",
    "    for i in range(len(sents_moby)):\n",
    "        a = a + len(nltk.word_tokenize(sents_moby[i]))\n",
    "    num_sent = len(sents_moby)\n",
    "    return a / num_sent\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "def answer_eight():\n",
    "    \n",
    "    dist_word = nltk.FreqDist(moby_tokens)\n",
    "    list_new = []\n",
    "    for w in dist_word.keys():\n",
    "        list_new.append((dist_word[w], w ))\n",
    "    list_new.sort(reverse = True)\n",
    "    net_list = []\n",
    "    for i in range(5):\n",
    "        net_list.append(list_new[:5][i][1])\n",
    "    pos_tag = nltk.pos_tag(net_list)\n",
    "    new_list_final = []\n",
    "    for i in range(5):\n",
    "        new_list_final.append((pos_tag[:5][i][1], list_new[:5][i][0]))\n",
    "    return new_list_final\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    # get first letter of each word with c\n",
    "    entry1 = [i for i in correct_spellings if i[0]=='c']\n",
    "    # calculate the distance of each word with entry and link both together\n",
    "    dist_one = [(nltk.jaccard_distance(set(nltk.ngrams(entries[0], n=3)), \\\n",
    "                                  set(nltk.ngrams(a, n=3))), a) for a in entry1]\n",
    "\n",
    "    entry2 = [i for i in correct_spellings if i[0]=='i']\n",
    "    dist_two = [(nltk.jaccard_distance(set(nltk.ngrams(entries[1], n=3)), \\\n",
    "                                  set(nltk.ngrams(a, n=3))), a) for a in entry2]\n",
    "\n",
    "    entry3 = [i for i in correct_spellings if i[0]=='v']\n",
    "    dist_three = [(nltk.jaccard_distance(set(nltk.ngrams(entries[2], n=3)), \\\n",
    "                                  set(nltk.ngrams(a, n=3))), a) for a in entry3]\n",
    "\n",
    "    # sort them to ascending order so shortest distance is on top.\n",
    "    # extract the word only\n",
    "    answer = [sorted(dist_one)[0][1], sorted(dist_two)[0][1], sorted(dist_three)[0][1]]\n",
    "\n",
    "    return answer\n",
    "\n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    # get first letter of each word with c\n",
    "    entry1 = [i for i in correct_spellings if i[0]=='c']\n",
    "    # calculate the distance of each word with entry and link both together\n",
    "    dist_one = [(nltk.jaccard_distance(set(nltk.ngrams(entries[0], n=4)), \\\n",
    "                                  set(nltk.ngrams(a, n=4))), a) for a in entry1]\n",
    "\n",
    "    entry2 = [i for i in correct_spellings if i[0]=='i']\n",
    "    dist_two = [(nltk.jaccard_distance(set(nltk.ngrams(entries[1], n=4)), \\\n",
    "                                  set(nltk.ngrams(a, n=4))), a) for a in entry2]\n",
    "\n",
    "    entry3 = [i for i in correct_spellings if i[0]=='v']\n",
    "    dist_three = [(nltk.jaccard_distance(set(nltk.ngrams(entries[2], n=4)), \\\n",
    "                                  set(nltk.ngrams(a, n=4))), a) for a in entry3]\n",
    "\n",
    "    # sort them to ascending order so shortest distance is on top.\n",
    "    # extract the word only\n",
    "    answer = [sorted(dist_one)[0][1], sorted(dist_two)[0][1], sorted(dist_three)[0][1]]\n",
    "\n",
    "    return answer\n",
    "\n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    from nltk.corpus import words\n",
    "\n",
    "    correct_spellings = words.words()\n",
    "    # get first letter of each word with c\n",
    "    entry1 = [i for i in correct_spellings if i[0]=='c']\n",
    "    # calculate the distance of each word with entry and link both together\n",
    "    dist_one = [((nltk.edit_distance(entries[0], a)), a) for a in entry1]\n",
    "\n",
    "    entry2 = [i for i in correct_spellings if i[0]=='i']\n",
    "    dist_two = [((nltk.edit_distance(entries[1], a)), a) for a in entry2]\n",
    "\n",
    "    entry3 = [i for i in correct_spellings if i[0]=='v']\n",
    "    dist_three = [((nltk.edit_distance(entries[2], a)), a) for a in entry3]\n",
    "\n",
    "    # sort them to ascending order so shortest distance is on top.\n",
    "    # extract the word only\n",
    "    answer = [sorted(dist_one)[0][1], sorted(dist_two)[0][1], sorted(dist_three)[0][1]]\n",
    "\n",
    "    return answer\n",
    "    \n",
    "answer_eleven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
